{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "423670b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Using scaled EDA dataset for feature engineering\n"
     ]
    }
   ],
   "source": [
    "# Import Required Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import os\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "ROOT = os.path.dirname(os.path.abspath('scaled_eda_data.csv'))\n",
    "DATA_PATH = 'scaled_eda_data.csv'\n",
    "TARGET = 'is_late'\n",
    "print(' Using scaled EDA dataset for feature engineering')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbccfe81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (2832, 19)\n",
      "Source: scaled_eda_data.csv (from EDA Analysis)\n",
      "\n",
      "Column data types:\n",
      "float64    10\n",
      "object      7\n",
      "bool        2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n",
      "MISSING VALUES (Top 20 columns):\n",
      "============================================================\n",
      "restaurant_avg_prep_min    141\n",
      "delivery_partner_orders    133\n",
      "order_value_inr            125\n",
      "distance_km                123\n",
      "delivery_partner_rating     93\n",
      "restaurant_rating           80\n",
      "id                           0\n",
      "day_of_week                  0\n",
      "order_hour                   0\n",
      "cuisine_type                 0\n",
      "restaurant_type              0\n",
      "weather_condition            0\n",
      "is_peak_hour                 0\n",
      "num_items                    0\n",
      "estimated_delivery_min       0\n",
      "traffic_density              0\n",
      "is_promo_order               0\n",
      "area_type                    0\n",
      "is_late                      0\n",
      "dtype: int64\n",
      "\n",
      "Total columns with missing values: 6\n",
      "\n",
      "============================================================\n",
      "TARGET VARIABLE DISTRIBUTION:\n",
      "============================================================\n",
      "is_late\n",
      "on_time    1717\n",
      "late       1115\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Total records: 2832\n",
      "\n",
      "Class Imbalance Ratio: 1.54:1\n",
      "  Class imbalance detected! SMOTE will be applied.\n"
     ]
    }
   ],
   "source": [
    "# Load the scaled data from EDA\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Dataset shape:', df.shape)\n",
    "print('Source: scaled_eda_data.csv (from EDA Analysis)')\n",
    "print('\\nColumn data types:')\n",
    "print(df.dtypes.value_counts())\n",
    "print('\\n' + '='*60)\n",
    "print('MISSING VALUES (Top 20 columns):')\n",
    "print('='*60)\n",
    "missing_counts = df.isnull().sum().sort_values(ascending=False)\n",
    "if missing_counts.sum() > 0:\n",
    "    print(missing_counts.head(20))\n",
    "    print(f'\\nTotal columns with missing values: {missing_counts[missing_counts > 0].shape[0]}')\n",
    "else:\n",
    "    print(' No missing values found (already handled in EDA)')\n",
    "\n",
    "# Analyze target variable\n",
    "print('\\n' + '='*60)\n",
    "print('TARGET VARIABLE DISTRIBUTION:')\n",
    "print('='*60)\n",
    "print(df[TARGET].value_counts(dropna=False))\n",
    "print(f'\\nTotal records: {len(df)}')\n",
    "\n",
    "# Calculate class imbalance ratio\n",
    "value_counts = df[TARGET].value_counts()\n",
    "max_class = value_counts.max()\n",
    "min_class = value_counts.min()\n",
    "imbalance_ratio = max_class / min_class\n",
    "print(f'\\nClass Imbalance Ratio: {imbalance_ratio:.2f}:1')\n",
    "\n",
    "if imbalance_ratio > 1.5:\n",
    "    apply_smote = True\n",
    "    print('  Class imbalance detected! SMOTE will be applied.')\n",
    "else:\n",
    "    apply_smote = False\n",
    "    print(' Data is relatively balanced. SMOTE may not be necessary.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bceb5cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MISSING VALUE CHECK:\n",
      "============================================================\n",
      "Total records: 2832\n",
      "Total missing values in dataset: 695\n",
      "\n",
      "Records after removing ALL missing values: 2185\n",
      "Data retention: 77.15%\n",
      "\n",
      " Strategy: Remove rows with 4 or more missing fields\n",
      "  Reason: 77.15% data retention (< 95%)\n",
      "  Rows dropped: 0\n",
      "\n",
      "Records after cleaning: 2832\n",
      "Data retention: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Missing value check (should be minimal from EDA)\n",
    "print('='*60)\n",
    "print('MISSING VALUE CHECK:')\n",
    "print('='*60)\n",
    "\n",
    "total_records = len(df)\n",
    "missing_total = df.isnull().sum().sum()\n",
    "print(f'Total records: {total_records}')\n",
    "print(f'Total missing values in dataset: {missing_total}')\n",
    "\n",
    "if missing_total == 0:\n",
    "    print('\\n No missing values - dataset already cleaned by EDA pipeline')\n",
    "    df_clean = df.copy()\n",
    "    strategy_used = 'Pre-cleaned by EDA'\n",
    "else:\n",
    "    # Strategy 1: Remove all rows with any missing values\n",
    "    df_no_missing = df.dropna()\n",
    "    retention_pct = (len(df_no_missing) / total_records) * 100\n",
    "    \n",
    "    print(f'\\nRecords after removing ALL missing values: {len(df_no_missing)}')\n",
    "    print(f'Data retention: {retention_pct:.2f}%')\n",
    "    \n",
    "    # Decide strategy\n",
    "    if retention_pct >= 95:\n",
    "        print('\\n Strategy: Remove all rows with ANY missing values')\n",
    "        print(f'  Reason: {retention_pct:.2f}% data retention (>= 95%)')\n",
    "        df_clean = df.dropna()\n",
    "        rows_dropped = total_records - len(df_clean)\n",
    "        print(f'  Rows dropped: {rows_dropped}')\n",
    "        strategy_used = 'Drop all missing'\n",
    "    else:\n",
    "        print('\\n Strategy: Remove rows with 4 or more missing fields')\n",
    "        print(f'  Reason: {retention_pct:.2f}% data retention (< 95%)')\n",
    "        # Count missing values per row\n",
    "        missing_per_row = df.isnull().sum(axis=1)\n",
    "        df_clean = df[missing_per_row < 4]\n",
    "        rows_dropped = total_records - len(df_clean)\n",
    "        print(f'  Rows dropped: {rows_dropped}')\n",
    "        strategy_used = 'Drop rows with 4+ missing'\n",
    "\n",
    "print(f'\\nRecords after cleaning: {len(df_clean)}')\n",
    "print(f'Data retention: {(len(df_clean) / total_records) * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b7df015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CLASS DISTRIBUTION BEFORE SMOTE:\n",
      "============================================================\n",
      "1    1717\n",
      "0    1115\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Applying SMOTE to balance classes...\n",
      " SMOTE applied successfully!\n",
      "\n",
      "CLASS DISTRIBUTION AFTER SMOTE:\n",
      "1    1717\n",
      "0    1717\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for SMOTE and feature engineering\n",
    "y = df_clean[TARGET].copy()\n",
    "X = df_clean.drop(columns=[TARGET])\n",
    "\n",
    "# Encode target variable if necessary\n",
    "if y.dtype == object or y.dtype.name == 'category':\n",
    "    le_target = LabelEncoder()\n",
    "    y = pd.Series(le_target.fit_transform(y.astype(str)), index=y.index)\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('CLASS DISTRIBUTION BEFORE SMOTE:')\n",
    "print('='*60)\n",
    "print(y.value_counts())\n",
    "\n",
    "# Apply SMOTE if needed\n",
    "if apply_smote:\n",
    "    print('\\nApplying SMOTE to balance classes...')\n",
    "    # Identify numeric and categorical columns\n",
    "    numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "    \n",
    "    # Impute before SMOTE (SMOTE works on numeric data)\n",
    "    if numeric_cols:\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        X_num = pd.DataFrame(num_imputer.fit_transform(X[numeric_cols]), columns=numeric_cols)\n",
    "    \n",
    "    if cat_cols:\n",
    "        X_cat = X[cat_cols].fillna('missing')\n",
    "        X_cat = pd.get_dummies(X_cat, drop_first=True)\n",
    "    \n",
    "    X_processed = pd.concat([X_num, X_cat], axis=1) if cat_cols else X_num\n",
    "    X_processed = X_processed.fillna(0)\n",
    "    \n",
    "    # Apply SMOTE\n",
    "    smote = SMOTE(random_state=42, k_neighbors=5)\n",
    "    try:\n",
    "        X_smoted, y_smoted = smote.fit_resample(X_processed, y)\n",
    "        X_smoted = pd.DataFrame(X_smoted, columns=X_processed.columns)\n",
    "        y_smoted = pd.Series(y_smoted, index=range(len(y_smoted)))\n",
    "        print(' SMOTE applied successfully!')\n",
    "        print('\\nCLASS DISTRIBUTION AFTER SMOTE:')\n",
    "        print(y_smoted.value_counts())\n",
    "        \n",
    "        # Update for next sections\n",
    "        X = X_smoted\n",
    "        y = y_smoted\n",
    "        df_for_modeling = X.copy()\n",
    "        df_for_modeling.insert(0, TARGET, y.values)\n",
    "    except Exception as e:\n",
    "        print(f'⚠️  SMOTE application failed: {e}')\n",
    "        print('Proceeding without SMOTE')\n",
    "        df_for_modeling = df_clean.copy()\n",
    "else:\n",
    "    print('\\nSMOTE not applied (no significant class imbalance detected)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2e9132f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FEATURE IMPORTANCE WITH RIDGE REGRESSION:\n",
      "============================================================\n",
      "\n",
      "Numeric columns: 10\n",
      "Categorical columns: 32\n",
      "\n",
      "Processed feature count: 42\n",
      "\n",
      "Fitting LogisticRegression with Ridge (L2) penalty...\n",
      " Ridge model fitted successfully!\n",
      "\n",
      "Top 20 Features by Ridge Coefficient:\n",
      "                           Feature  Coefficient  Abs_Coefficient\n",
      "    cuisine_type_pizza_burger_True    -0.444030         0.444030\n",
      "          cuisine_type_indian_True    -0.413575         0.413575\n",
      "         cuisine_type_dessert_True    -0.397745         0.397745\n",
      "    restaurant_type_fast_food_True    -0.395602         0.395602\n",
      "    cuisine_type_south_indian_True    -0.374158         0.374158\n",
      "         cuisine_type_chinese_True    -0.373291         0.373291\n",
      "restaurant_type_casual_dining_True    -0.364227         0.364227\n",
      "restaurant_type_cloud_kitchen_True    -0.330676         0.330676\n",
      "           cuisine_type_other_True    -0.320017         0.320017\n",
      "         cuisine_type_italian_True    -0.309445         0.309445\n",
      "  restaurant_type_fine_dining_True    -0.300209         0.300209\n",
      " weather_condition_heavy_rain_True    -0.276945         0.276945\n",
      "         day_of_week_thursday_True    -0.263452         0.263452\n",
      "         cuisine_type_healthy_True    -0.234206         0.234206\n",
      "           day_of_week_monday_True    -0.222855         0.222855\n",
      "           day_of_week_sunday_True    -0.222240         0.222240\n",
      "        weather_condition_fog_True    -0.221711         0.221711\n",
      "                 is_peak_hour_True    -0.218315         0.218315\n",
      "         day_of_week_saturday_True    -0.216039         0.216039\n",
      "        day_of_week_wednesday_True    -0.184622         0.184622\n"
     ]
    }
   ],
   "source": [
    "# Prepare features for Ridge Regression\n",
    "print('='*60)\n",
    "print('FEATURE IMPORTANCE WITH RIDGE REGRESSION:')\n",
    "print('='*60)\n",
    "\n",
    "if not apply_smote:\n",
    "    # If SMOTE was not applied, prepare data now\n",
    "    y = df_clean[TARGET].copy()\n",
    "    X = df_clean.drop(columns=[TARGET])\n",
    "    \n",
    "    if y.dtype == object or y.dtype.name == 'category':\n",
    "        le_target = LabelEncoder()\n",
    "        y = pd.Series(le_target.fit_transform(y.astype(str)), index=y.index)\n",
    "\n",
    "# Identify numeric and categorical columns\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "cat_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(f'\\nNumeric columns: {len(numeric_cols)}')\n",
    "print(f'Categorical columns: {len(cat_cols)}')\n",
    "\n",
    "# Handle numeric columns - impute with median\n",
    "X_numeric = X[numeric_cols].copy() if numeric_cols else pd.DataFrame()\n",
    "if numeric_cols:\n",
    "    num_imputer = SimpleImputer(strategy='median')\n",
    "    X_numeric = pd.DataFrame(num_imputer.fit_transform(X_numeric), columns=numeric_cols)\n",
    "\n",
    "# Handle categorical columns - impute and one-hot encode\n",
    "X_categorical = pd.DataFrame() if not cat_cols else X[cat_cols].fillna('missing')\n",
    "if cat_cols:\n",
    "    X_categorical = pd.get_dummies(X_categorical.astype(str), drop_first=True)\n",
    "\n",
    "# Combine processed features\n",
    "if X_categorical.shape[1] > 0:\n",
    "    X_processed = pd.concat([X_numeric, X_categorical], axis=1)\n",
    "else:\n",
    "    X_processed = X_numeric\n",
    "\n",
    "# Fill any remaining NaNs\n",
    "X_processed = X_processed.fillna(0)\n",
    "print(f'\\nProcessed feature count: {X_processed.shape[1]}')\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "# Fit Ridge Regression (L2 penalty) for feature importance\n",
    "print('\\nFitting LogisticRegression with Ridge (L2) penalty...')\n",
    "try:\n",
    "    ridge_model = LogisticRegressionCV(cv=5, penalty='l2', solver='lbfgs', \n",
    "                                       scoring='roc_auc', max_iter=5000, n_jobs=-1)\n",
    "    ridge_model.fit(X_scaled, y)\n",
    "    ridge_coef = ridge_model.coef_.ravel()\n",
    "    print(' Ridge model fitted successfully!')\n",
    "    \n",
    "    # Create coefficient dataframe\n",
    "    coef_df = pd.DataFrame({\n",
    "        'Feature': X_processed.columns,\n",
    "        'Coefficient': ridge_coef,\n",
    "        'Abs_Coefficient': np.abs(ridge_coef)\n",
    "    })\n",
    "    coef_df = coef_df.sort_values('Abs_Coefficient', ascending=False)\n",
    "    \n",
    "    print('\\nTop 20 Features by Ridge Coefficient:')\n",
    "    print(coef_df.head(20).to_string(index=False))\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'⚠️  Ridge model fitting failed: {e}')\n",
    "    coef_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21f6bc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "LOW IMPORTANCE FEATURE REMOVAL:\n",
      "============================================================\n",
      "\n",
      "10th Percentile Threshold: 0.017687\n",
      "\n",
      "Total features to drop: 5\n",
      "Features to keep: 37\n",
      "\n",
      "============================================================\n",
      "FEATURES BEING DROPPED:\n",
      "============================================================\n",
      "                Feature  Abs_Coefficient\n",
      " estimated_delivery_min         0.016190\n",
      "delivery_partner_rating         0.014260\n",
      "                     id         0.014247\n",
      "              num_items         0.013586\n",
      "        order_value_inr         0.009220\n",
      "\n",
      " Features removed. Final feature count: 37\n"
     ]
    }
   ],
   "source": [
    "# Drop low importance features\n",
    "print('='*60)\n",
    "print('LOW IMPORTANCE FEATURE REMOVAL:')\n",
    "print('='*60)\n",
    "\n",
    "if coef_df is not None:\n",
    "    # Calculate 10th percentile threshold\n",
    "    threshold = np.percentile(coef_df['Abs_Coefficient'], 10)\n",
    "    print(f'\\n10th Percentile Threshold: {threshold:.6f}')\n",
    "    \n",
    "    # Identify features to drop\n",
    "    features_to_drop = coef_df[coef_df['Abs_Coefficient'] <= threshold]['Feature'].tolist()\n",
    "    print(f'\\nTotal features to drop: {len(features_to_drop)}')\n",
    "    print(f'Features to keep: {len(coef_df) - len(features_to_drop)}')\n",
    "    \n",
    "    print('\\n' + '='*60)\n",
    "    print('FEATURES BEING DROPPED:')\n",
    "    print('='*60)\n",
    "    drop_info = coef_df[coef_df['Abs_Coefficient'] <= threshold][['Feature', 'Abs_Coefficient']]\n",
    "    print(drop_info.to_string(index=False))\n",
    "    \n",
    "    # Drop features from processed data\n",
    "    X_final = X_processed.drop(columns=features_to_drop)\n",
    "    print(f'\\n Features removed. Final feature count: {X_final.shape[1]}')\n",
    "    \n",
    "else:\n",
    "    print('  Ridge model not available. Using all features.')\n",
    "    features_to_drop = []\n",
    "    X_final = X_processed.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "432e3577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SAVING ENGINEERED DATASET:\n",
      "============================================================\n",
      "\n",
      " Dataset saved successfully!\n",
      "File: engineered_features.xlsx\n",
      "Shape: (3434, 38)\n",
      "Rows: 3434, Columns: 38\n",
      "\n",
      "============================================================\n",
      "SUMMARY STATISTICS:\n",
      "============================================================\n",
      "Source dataset: scaled_eda_data.csv\n",
      "After missing value check: (2832, 19)\n",
      "After SMOTE: 3434 rows\n",
      "Final engineered dataset: (3434, 38)\n",
      "Features removed: 5\n",
      "Features retained: 37\n",
      "\n",
      " Feature importance scores saved to \"Feature_Importance\" sheet\n",
      "\n",
      " Feature Engineering Pipeline Complete!\n",
      "Pipeline: scaled_eda_data.csv → SMOTE → Ridge Selection → engineered_features.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Save the engineered dataset to Excel\n",
    "print('='*60)\n",
    "print('SAVING ENGINEERED DATASET:')\n",
    "print('='*60)\n",
    "\n",
    "# Create final dataframe with target variable\n",
    "df_final = X_final.copy()\n",
    "df_final.insert(0, TARGET, y.values)\n",
    "\n",
    "# Define output file path\n",
    "output_file = 'engineered_features.xlsx'\n",
    "\n",
    "try:\n",
    "    # Save to Excel\n",
    "    df_final.to_excel(output_file, index=False, sheet_name='Features')\n",
    "    print(f'\\n Dataset saved successfully!')\n",
    "    print(f'File: {output_file}')\n",
    "    print(f'Shape: {df_final.shape}')\n",
    "    print(f'Rows: {df_final.shape[0]}, Columns: {df_final.shape[1]}')\n",
    "    \n",
    "    # Summary statistics\n",
    "    print('\\n' + '='*60)\n",
    "    print('SUMMARY STATISTICS:')\n",
    "    print('='*60)\n",
    "    print(f'Source dataset: scaled_eda_data.csv')\n",
    "    print(f'After missing value check: {df_clean.shape}')\n",
    "    if apply_smote:\n",
    "        print(f'After SMOTE: {df_final.shape[0]} rows')\n",
    "    print(f'Final engineered dataset: {df_final.shape}')\n",
    "    print(f'Features removed: {len(features_to_drop)}')\n",
    "    print(f'Features retained: {df_final.shape[1] - 1}')\n",
    "    \n",
    "    if coef_df is not None:\n",
    "        # Save coefficient information to separate sheet\n",
    "        with pd.ExcelWriter(output_file, mode='a', engine='openpyxl') as writer:\n",
    "            coef_df.to_excel(writer, sheet_name='Feature_Importance', index=False)\n",
    "        print(f'\\n Feature importance scores saved to \"Feature_Importance\" sheet')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'  Error saving file: {e}')\n",
    "\n",
    "print('\\n Feature Engineering Pipeline Complete!')\n",
    "print('Pipeline: scaled_eda_data.csv → SMOTE → Ridge Selection → engineered_features.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
